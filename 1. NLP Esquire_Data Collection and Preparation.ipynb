{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Master of Data Science (Digital Humanities)\n",
    "#### Final Project Notebooks\n",
    "\n",
    "### DATA40345 Data Science Research Project\n",
    "\n",
    "#### NLP Esquire: A Data-Driven Analysis and Categorisation of the Judgments of the United Kingdom Supreme Court"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 1: Data Collection and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA COLLECTION\n",
    "Scraping UK Supreme Court judgements from The Supreme Court website\n",
    "(https://www.supremecourt.uk/decided-cases/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time #to add sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.supremecourt.uk/decided-cases/2009.html\n",
      "https://www.supremecourt.uk/decided-cases/2010.html\n",
      "https://www.supremecourt.uk/decided-cases/2011.html\n",
      "https://www.supremecourt.uk/decided-cases/2012.html\n",
      "https://www.supremecourt.uk/decided-cases/2013.html\n",
      "https://www.supremecourt.uk/decided-cases/2014.html\n",
      "https://www.supremecourt.uk/decided-cases/2015.html\n",
      "https://www.supremecourt.uk/decided-cases/2016.html\n",
      "https://www.supremecourt.uk/decided-cases/2017.html\n",
      "https://www.supremecourt.uk/decided-cases/2018.html\n",
      "https://www.supremecourt.uk/decided-cases/2019.html\n",
      "https://www.supremecourt.uk/decided-cases/2020.html\n",
      "https://www.supremecourt.uk/decided-cases/2021.html\n",
      "https://www.supremecourt.uk/decided-cases/2022.html\n"
     ]
    }
   ],
   "source": [
    "#Judgements from 2009-2023 available online\n",
    "#for loop to get links to the pages with the judgments (for each year)\n",
    "\n",
    "urlWebsite = \"https://www.supremecourt.uk/decided-cases/\" \n",
    "urls = [] #empty list to store full url to each year\n",
    "\n",
    "url_index = \"https://www.supremecourt.uk/decided-cases/index.html\" #includes the latest judgments\n",
    "\n",
    "for i in range(2009,2023):\n",
    "    strUrl = urlWebsite + str(i) + \".html\"\n",
    "    print(strUrl)\n",
    "    urls.append(strUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.supremecourt.uk/decided-cases/2009.html', 'https://www.supremecourt.uk/decided-cases/2010.html', 'https://www.supremecourt.uk/decided-cases/2011.html', 'https://www.supremecourt.uk/decided-cases/2012.html', 'https://www.supremecourt.uk/decided-cases/2013.html', 'https://www.supremecourt.uk/decided-cases/2014.html', 'https://www.supremecourt.uk/decided-cases/2015.html', 'https://www.supremecourt.uk/decided-cases/2016.html', 'https://www.supremecourt.uk/decided-cases/2017.html', 'https://www.supremecourt.uk/decided-cases/2018.html', 'https://www.supremecourt.uk/decided-cases/2019.html', 'https://www.supremecourt.uk/decided-cases/2020.html', 'https://www.supremecourt.uk/decided-cases/2021.html', 'https://www.supremecourt.uk/decided-cases/2022.html']\n"
     ]
    }
   ],
   "source": [
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to obtain the judgments in each case:\n",
    "#obtaining the relative url to each case in each year\n",
    "\n",
    "case_links = [] #empty list to store the (relative) links to each case in each year\n",
    "\n",
    "for i in urls:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    test_links = soup.find_all(class_=\"fourthColumn\")\n",
    "    #print(test_links)\n",
    "\n",
    "    for link in test_links:\n",
    "        case = link.find(\"a\", class_=\"more\") #get relative link\n",
    "        if (case != None):\n",
    "            #print(case[\"href\"])\n",
    "        case_links.append(case[\"href\"])\n",
    "        time.sleep(2)\n",
    "\n",
    "#all webpages with specific years (2022-2009) have identical html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While 2009-2022 judgments are in webpages that have identical hmtl coding, the most recent judgments i.e.2023 judgments are in a webpage with different html. Specifically class=\"fifthColumn\" and not \"fourthColumn\". Therefore, 2023 needs to be scraped separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the latest judgments i.e. 2023 judgments are in a webpage with different html.\n",
    "#specifically class=\"fifthColumn\" and not \"fourthColumn\"\n",
    "#therefore, 2023 needs to be scraped separately.\n",
    "#get links for index page\n",
    "url_index = \"https://www.supremecourt.uk/decided-cases/index.html\" #includes the latest judgments\n",
    "\n",
    "#to obtain the judgments in each case in index year:\n",
    "#obtaining the relative url to each case:\n",
    "\n",
    "page = requests.get(url_index)\n",
    "soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "test_links = soup.find_all(class_=\"fifthColumn\")\n",
    "\n",
    "for link in test_links:\n",
    "    case = link.find(\"a\", class_=\"more\") #get relative link\n",
    "    if (case != None):\n",
    "        #print(case[\"href\"])\n",
    "    case_links.append(case[\"href\"])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the global url for each case\n",
    "\n",
    "globalurls = [] #empty list to store the global urls\n",
    "globurl = \"https://www.supremecourt.uk\"\n",
    "\n",
    "for i in case_links:\n",
    "    caseLink = globurl + i #appending the relative links to the global url\n",
    "    #print(caseLink)\n",
    "    globalurls.append(caseLink) #appending all global urls to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalurls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the Judgment Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judgment_links = []\n",
    "for i in globalurls:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    text = soup.find(\"a\", title=\"Judgment (PDF)\")\n",
    "    if (text != None):\n",
    "        #print(text[\"href\"]) \n",
    "        judgment_links.append(text[\"href\"])\n",
    "    else:\n",
    "        print(\"The Judgment in\", i, \"is not available\")\n",
    "    time.sleep(3) #sleep is used to provide sufficient time between requests to not overload the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(judgment_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardising links\n",
    "\n",
    "a = \"https://www.supremecourt.uk/cases/\"\n",
    "b = \"https://www.supremecourt.uk/\"\n",
    "\n",
    "#preparing the global urls for the judgments\n",
    "judgmentUrls = [] #list to store the global urls to all the judgement PDFs\n",
    "\n",
    "for i in judgment_links:\n",
    "    if \"/cases/\" in i:\n",
    "        judgmentUrls.append(b + i)\n",
    "    else:\n",
    "        judgmentUrls.append(a + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(judgmentUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(judgmentUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#issues noted during scraping:\n",
    "    #some of the urls have \"/cases/docs/\" while others have only \"docs/\"\n",
    "    #one link is scraped with spaces\n",
    "    #therefore, the relative url is standardised below\n",
    "\n",
    "standardised_links = []\n",
    "\n",
    "for i in judgmentUrls:\n",
    "    #standardised_link = i.replace(\"/cases/\", \"\")\n",
    "    standardised_link2 = i.replace(\"https://www.supremecourt.uk/cases/docs/uksc-2018-0091-judgment minus restrictions.pdf\",\"https://www.supremecourt.uk/cases/docs/uksc-2018-0091-judgment%20minus%20restrictions.pdf\" )\n",
    "    standardised_links.append(standardised_link2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(standardised_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the global urls for the judgments\n",
    "\n",
    "global_judgementUrl = \"https://www.supremecourt.uk/cases/\"\n",
    "\n",
    "judgmentUrls = [] #list to store the global urls to all the judgement PDFs\n",
    "\n",
    "for i in standardised_links:\n",
    "    judgmentLink = global_judgementUrl + i #appending the relative links to the global url\n",
    "    judgmentUrls.append(judgmentLink) #appending all global urls to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop to save the pdf files \n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "uksc_folder = \"./NEW UKSC PDFMINER\"\n",
    "if not os.path.exists(uksc_folder): #check if \"Dataset_UKSC folder exists\"\n",
    "    os.makedirs(uksc_folder) #if it does not, create a new folder called titled Dataset_UKSC\n",
    "\n",
    "for i in standardised_links:\n",
    "    filename = i.split(\"/\")[-1] #[-1 indicates that the last portion divided by \"/\" is to be used to name]\n",
    "    filepath = os.path.join(uksc_folder, filename)\n",
    "    urllib.request.urlretrieve(i, filepath)\n",
    "    time.sleep(3)\n",
    "\n",
    "#https://www.tutorialspoint.com/downloading-files-from-web-using-python\n",
    "#used to identify how to extract filename from link\n",
    "\n",
    "#https://docs.python.org/3/library/urllib.request.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping metadata of UK Supreme Court judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data for quantitative analysis\n",
    "#the html coding for index year (2023), 2022, and 2021 are identical\n",
    "#it also has the same information.\n",
    "#subsequent years do not have as much information.\n",
    "#therefore, the case links for each case 2023-2021 are first extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.supremecourt.uk/decided-cases/2021.html\n",
      "https://www.supremecourt.uk/decided-cases/2022.html\n"
     ]
    }
   ],
   "source": [
    "#extracting links for 2022 and 2021 first as 2023(index) has a different html structure\n",
    "\n",
    "#for loop to get links to the pages with the judgments (for each year)\n",
    "\n",
    "urlWebsite = \"https://www.supremecourt.uk/decided-cases/\" \n",
    "urls_22_21 = [] #empty list to store full url to each year\n",
    "\n",
    "#url_index = \"https://www.supremecourt.uk/decided-cases/index.html\" #includes the latest judgments\n",
    "\n",
    "for i in range(2021,2023):\n",
    "    strUrl = urlWebsite + str(i) + \".html\"\n",
    "    print(strUrl)\n",
    "    urls_22_21.append(strUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_22_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to obtain the judgments in each case in 2022 and 2021\n",
    "#obtaining the relative url to each case in each year\n",
    "\n",
    "case_links_22_21 = [] #empty list to store the (relative) links to each case in each year\n",
    "\n",
    "for i in urls_22_21:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    test_links = soup.find_all(class_=\"fourthColumn\")\n",
    "    #print(test_links)\n",
    "\n",
    "    for link in test_links:\n",
    "        case = link.find(\"a\", class_=\"more\") #get relative link\n",
    "        if (case != None):\n",
    "            #print(case[\"href\"])\n",
    "        case_links_22_21.append(case[\"href\"])\n",
    "        time.sleep(2)\n",
    "\n",
    "#all webpages with specific years (2022-2009) have identical html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_links_22_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cases/uksc-2021-0078.html\n",
      "/cases/uksc-2021-0087.html\n",
      "/cases/uksc-2021-0149.html\n",
      "/cases/uksc-2022-0075.html\n",
      "/cases/uksc-2020-0208.html\n",
      "/cases/uksc-2021-0195.html\n",
      "/cases/uksc-2021-0159.html\n",
      "/cases/uksc-2021-0125.html\n",
      "/cases/uksc-2021-0038.html\n",
      "/cases/uksc-2021-0144.html\n",
      "/cases/uksc-2021-0138.html\n",
      "/cases/uksc-2021-0056.html\n",
      "/cases/uksc-2021-0050.html\n",
      "/cases/uksc-2021-0089.html\n",
      "/cases/uksc-2022-0056.html\n",
      "/cases/uksc-2022-0052.html\n",
      "/cases/uksc-2021-0216.html\n",
      "/cases/uksc-2018-0192.html\n",
      "/cases/uksc-2018-0191.html\n",
      "/cases/uksc-2021-0019.html\n",
      "/cases/uksc-2021-0188.html\n",
      "/cases/uksc-2021-0031.html\n",
      "/cases/uksc-2021-0047.html\n",
      "/cases/uksc-2021-0059.html\n",
      "/cases/uksc-2022-0089.html\n",
      "/cases/uksc-2022-0093.html\n",
      "/cases/uksc-2020-0056.html\n",
      "/cases/uksc-2020-0002.html\n",
      "/cases/uksc-2021-0027.html\n",
      "/cases/uksc-2021-0028.html\n"
     ]
    }
   ],
   "source": [
    "#the latest judgments i.e. 2023 judgments are in a webpage with different html.\n",
    "#specifically class=\"fifthColumn\" and not \"fourthColumn\"\n",
    "#therefore, 2023 needs to be scraped separately.\n",
    "#get links for index page\n",
    "url_index = \"https://www.supremecourt.uk/decided-cases/index.html\" #includes the latest judgments\n",
    "\n",
    "#to obtain the judgments in each case in index year:\n",
    "#obtaining the relative url to each case:\n",
    "\n",
    "page = requests.get(url_index)\n",
    "soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "test_links = soup.find_all(class_=\"fifthColumn\")\n",
    "\n",
    "for link in test_links:\n",
    "    case = link.find(\"a\", class_=\"more\") #get relative link\n",
    "    if (case != None):\n",
    "        print(case[\"href\"])\n",
    "    case_links_22_21.append(case[\"href\"])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_links_22_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the global url for each case\n",
    "\n",
    "globalurls1 = [] #empty list to store the global urls\n",
    "globurl = \"https://www.supremecourt.uk\"\n",
    "\n",
    "for i in case_links_22_21:\n",
    "    caseLink = globurl + i #appending the relative links to the global url\n",
    "    globalurls1.append(caseLink) #appending all global urls to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalurls1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the years 2023-2021 have the same html code + information.\n",
    "#globalurls1 contains the case links to all cases in 2023-2021\n",
    "\n",
    "case_ref = [] \n",
    "first_part = \"https://www.supremecourt.uk/cases/\"\n",
    "last_part = \".html\"\n",
    "\n",
    "case_name = [] #empty list to store the names of the parties\n",
    "case_ID = [] #empty list to store the Case ID (i.e. the case in which the judgment is delivered)\n",
    "issue = []\n",
    "judgment_appealed = []\n",
    "justices = []\n",
    "hearing_start = []\n",
    "hearing_finish = []\n",
    "judgment_date = [] #empty list to store the date of delivery of judgment\n",
    "citation = []\n",
    "press_summary = []\n",
    "\n",
    "for i in globalurls1:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    mod_ref = i.replace(first_part, \"\").replace(last_part, \"\")\n",
    "    print(mod_ref)\n",
    "    case_ref.append(mod_ref)\n",
    "    \n",
    "    parties = soup.find(\"h2\")\n",
    "    if(parties !=None):\n",
    "        for name in parties:\n",
    "            name1 = name.text.strip()\n",
    "            #print(name1)\n",
    "            case_name.append(name1)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        case_name.append(\"N/A\")\n",
    "    \n",
    "    ids = soup.find(\"h3\", class_=\"sc-access\")\n",
    "    if (ids !=None):\n",
    "        for x in ids:\n",
    "            ids1 = x.text.strip()\n",
    "            #print(ids1)\n",
    "            case_ID.append(ids1)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        case_ID.append(\"N/A\")\n",
    "    \n",
    "    issues = soup.find(\"h4\", text=\"Issue\")\n",
    "    if(issues !=None):\n",
    "        for x in issues:\n",
    "            para = issues.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(text)    \n",
    "            issue.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        issue.append(\"N/A\")\n",
    "    \n",
    "    appeal = soup.find(\"h4\", text=\"Judgment appealed\")\n",
    "    if(appeal !=None):\n",
    "        for x in appeal:\n",
    "            para = appeal.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(text)    \n",
    "            judgment_appealed.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        judgment_appealed.append(\"N/A\")\n",
    "           \n",
    "    judges = soup.find(\"h4\", text=\"Justices\")\n",
    "    if(judges !=None):\n",
    "        for x in judges:\n",
    "            para = judges.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(text)    \n",
    "            justices.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        justices.append(\"N/A\")\n",
    "    \n",
    "    start = soup.find(\"h4\", text=\"Hearing start date\")\n",
    "    if(start !=None):\n",
    "        for x in start:\n",
    "            para = start.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(text)    \n",
    "            hearing_start.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        hearing_start.append(\"N/A\")\n",
    "    \n",
    "    finish = soup.find(\"h4\", text=\"Hearing finish date\")\n",
    "    if(finish !=None):\n",
    "        for x in finish:\n",
    "            para = finish.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(text)    \n",
    "            hearing_finish.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        hearing_finish.append(\"N/A\")\n",
    "    \n",
    "    judgment = soup.find(\"h4\", text=\"Judgment date\")\n",
    "    if(judgment !=None):\n",
    "        for x in judgment:\n",
    "            para = judgment.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(text)    \n",
    "            judgment_date.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        judgment_date.append(\"N/A\")\n",
    "\n",
    "    cite = soup.find(\"h4\", text=\"Neutral citation\")\n",
    "    if(cite !=None): \n",
    "        for x in cite:\n",
    "            para = cite.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(text)    \n",
    "            citation.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        citation.append(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Case References:  148\n",
      "No.of Case Names:  148\n",
      "No.of Case IDs:  148\n",
      "No.of Case Issues:  148\n",
      "No.of Judgments Appealed:  148\n",
      "No.of Case Judges:  148\n",
      "No.of Hearing Start Dates:  148\n",
      "No.of Hearing End Dates:  148\n",
      "No.of Judgement Delivery Dates:  148\n",
      "No.of Case Neutral Citations:  148\n"
     ]
    }
   ],
   "source": [
    "#ensure that the same no.of data for each list has been scraped.\n",
    "print(\"No.of Case References: \", len(case_ref))\n",
    "print(\"No.of Case Names: \", len(case_name))\n",
    "print(\"No.of Case IDs: \", len(case_ID))\n",
    "print(\"No.of Case Issues: \", len(issue))\n",
    "print(\"No.of Judgments Appealed: \", len(judgment_appealed))\n",
    "print(\"No.of Case Judges: \", len(justices))\n",
    "print(\"No.of Hearing Start Dates: \", len(hearing_start))\n",
    "print(\"No.of Hearing End Dates: \", len(hearing_finish))\n",
    "print(\"No.of Judgement Delivery Dates: \", len(judgment_date))\n",
    "print(\"No.of Case Neutral Citations: \", len(citation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Case References:  148\n",
      "No.of Case Names:  148\n",
      "No.of Case IDs:  148\n",
      "No.of Case Issues:  148\n",
      "No.of Judgments Appealed:  148\n",
      "No.of Case Judges:  148\n",
      "No.of Hearing Start Dates:  148\n",
      "No.of Hearing End Dates:  148\n",
      "No.of Judgement Delivery Dates:  148\n",
      "No.of Case Neutral Citations:  148\n"
     ]
    }
   ],
   "source": [
    "#ensure that the same no.of data for each list has been scraped.\n",
    "print(\"No.of Case References: \", len(case_ref))\n",
    "print(\"No.of Case Names: \", len(case_name))\n",
    "print(\"No.of Case IDs: \", len(case_ID))\n",
    "print(\"No.of Case Issues: \", len(issue))\n",
    "print(\"No.of Judgments Appealed: \", len(judgment_appealed))\n",
    "print(\"No.of Case Judges: \", len(justices))\n",
    "print(\"No.of Hearing Start Dates: \", len(hearing_start))\n",
    "print(\"No.of Hearing End Dates: \", len(hearing_finish))\n",
    "print(\"No.of Judgement Delivery Dates: \", len(judgment_date))\n",
    "print(\"No.of Case Neutral Citations: \", len(citation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data of 2021-2023 in a df (df1)\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(list(zip(case_ref, case_name, case_ID, issue, judgment_appealed, justices, hearing_start, hearing_finish, judgment_date, citation)),\n",
    "                         columns = [\"Case Ref\", \"Parties\", \"ID\", \"Issue\", \"Appealed Judgment\", \"Names of Judges\", \"Hearing Start Date\", \"Hearing End Data\", \"Date of Judgment\", \"Citation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148 entries, 0 to 147\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Case Ref            148 non-null    object\n",
      " 1   Parties             148 non-null    object\n",
      " 2   ID                  148 non-null    object\n",
      " 3   Issue               148 non-null    object\n",
      " 4   Appealed Judgment   148 non-null    object\n",
      " 5   Names of Judges     148 non-null    object\n",
      " 6   Hearing Start Date  148 non-null    object\n",
      " 7   Hearing End Data    148 non-null    object\n",
      " 8   Date of Judgment    148 non-null    object\n",
      " 9   Citation            148 non-null    object\n",
      "dtypes: object(10)\n",
      "memory usage: 11.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv for subsequent use\n",
    "df1.to_csv(\"UKSC 2021-23 Additional Data.csv\", encoding=\"utf-8\", index=False) #removes the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to scraping data from the individual judgment pages, data is also scraped from the press summaries as these provide the decision clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part = \"https://www.supremecourt.uk/cases/\"\n",
    "last_part = \".html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the links to the press summary webpages\n",
    "case_ref4 = []\n",
    "press_summary = []\n",
    "\n",
    "for i in globalurls1:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    mod_ref = i.replace(first_part, \"\").replace(last_part, \"\")\n",
    "    print(mod_ref)\n",
    "    case_ref4.append(mod_ref)\n",
    "\n",
    "#the press summary is coded in html in 4 different variations.\n",
    "#the following if-else is created to account for all 4 variations\n",
    "    press = soup.find(\"a\", title=\"Press Summary\")\n",
    "    if (press != None):\n",
    "        #print(press[\"href\"])\n",
    "        press_link = globurl + press[\"href\"]\n",
    "        #print(press_link)\n",
    "        press_summary.append(press_link)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        press1 = soup.find(\"a\", title=\"Press summary (HTML Version)\")\n",
    "        if (press1 != None):\n",
    "            press_link = globurl + press1[\"href\"]\n",
    "            #print(press_link)\n",
    "            press_summary.append(press_link)\n",
    "            time.sleep(1)\n",
    "        else: \n",
    "            press2 = soup.find(\"a\", title=\"Press summary HTML version\")\n",
    "            if (press2 !=None):\n",
    "                press_link = globurl + press2[\"href\"]\n",
    "                #print(press_link)\n",
    "                press_summary.append(press_link)\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                press3 = soup.find(\"a\", title=\"Press Summary HTML version\")\n",
    "                if (press3 !=None):\n",
    "                    press_link = globurl + press3[\"href\"]\n",
    "                    #print(press_link)\n",
    "                    press_summary.append(press_link)\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(\"Press Summary for Case\", mod_ref, \"is not available\")\n",
    "                    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(press_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only the decision from press summary html 2021-2023\n",
    "case_ref_decision = [] \n",
    "first_part1 = \"https://www.supremecourt.uk/press-summary/\"\n",
    "decision_only = []\n",
    "\n",
    "for i in press_summary:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    mod_caseref = i.replace(first_part1, \"\").replace(last_part, \"\")\n",
    "    #print(mod_caseref)\n",
    "    case_ref_decision.append(mod_caseref)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    decision = soup.find(class_=\"sc-access caption\", text=\"Judgment\")\n",
    "    if (decision !=None):\n",
    "        for x in decision:\n",
    "            para = decision.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(\"THIS IS THE DECISION IN\", mod_caseref, \":\", text)\n",
    "            decision_only.append(text)\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        decision_only.append(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_ref_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decision_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store decision in press summaries 2020-2023 in df6\n",
    "import pandas as pd\n",
    "df6 = pd.DataFrame(list(zip(case_ref_decision, decision_only)),\n",
    "                         columns = [\"Case Ref\", \"Decision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Ref</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uksc-2021-0160</td>\n",
       "      <td>Jointly, Lord Briggs, Lord Kitchin, Lord Burro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uksc-2020-0195</td>\n",
       "      <td>The Supreme Court unanimously allows the appea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uksc-2020-0081</td>\n",
       "      <td>\\r\\nThe Supreme Court unanimously dismisses th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uksc-2020-0029</td>\n",
       "      <td>The Supreme Court allows the appeals by the Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uksc-2020-0029</td>\n",
       "      <td>The Supreme Court allows the appeals by the Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Case Ref                                           Decision\n",
       "0  uksc-2021-0160  Jointly, Lord Briggs, Lord Kitchin, Lord Burro...\n",
       "1  uksc-2020-0195  The Supreme Court unanimously allows the appea...\n",
       "2  uksc-2020-0081  \\r\\nThe Supreme Court unanimously dismisses th...\n",
       "3  uksc-2020-0029  The Supreme Court allows the appeals by the Ch...\n",
       "4  uksc-2020-0029  The Supreme Court allows the appeals by the Ch..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv for subsequent use\n",
    "df6.to_csv(\"UKSC 2021-23 Decision Data.csv\", encoding=\"utf-8\", index=False) #removes the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data of press summaries 2020-2023 in df5\n",
    "import pandas as pd\n",
    "df5 = pd.DataFrame(list(zip(case_ref2, press_judges, background_info, reasons, decisions)),\n",
    "                         columns = [\"Case Ref\", \"Names of Judges\", \"Background\", \"Reasons\", \"Decision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Ref</th>\n",
       "      <th>Names of Judges</th>\n",
       "      <th>Background</th>\n",
       "      <th>Reasons</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uksc-2021-0160</td>\n",
       "      <td>Lord Briggs, Lady Arden, Lord Kitchin, Lord Bu...</td>\n",
       "      <td>Mr Crosland disclosed the outcome of a judgmen...</td>\n",
       "      <td>Jurisdiction</td>\n",
       "      <td>Jointly, Lord Briggs, Lord Kitchin, Lord Burro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uksc-2020-0195</td>\n",
       "      <td>Lord Reed (President), Lord Hodge (Deputy Pres...</td>\n",
       "      <td>The following issues arise. First, whether the...</td>\n",
       "      <td>The majority holds that section 13 of the Admi...</td>\n",
       "      <td>The Supreme Court unanimously allows the appea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uksc-2020-0081</td>\n",
       "      <td>Lord Reed (President), Lord Lloyd-Jones, Lady ...</td>\n",
       "      <td>Jointly, Lord Briggs, Lord Kitchin, Lord Burro...</td>\n",
       "      <td>Lady Arden considers that the Supreme Court do...</td>\n",
       "      <td>\\r\\nThe Supreme Court unanimously dismisses th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uksc-2020-0029</td>\n",
       "      <td>Lord Hodge (Deputy President), Lord Lloyd-Jone...</td>\n",
       "      <td>Jurisdiction</td>\n",
       "      <td>Merits</td>\n",
       "      <td>The Supreme Court allows the appeals by the Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uksc-2020-0029</td>\n",
       "      <td>Lord Hodge (Deputy President), Lord Lloyd-Jone...</td>\n",
       "      <td>The majority holds that section 13 of the Admi...</td>\n",
       "      <td>The First Instance Panel made no material erro...</td>\n",
       "      <td>The Supreme Court allows the appeals by the Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>uksc-2020-0206</td>\n",
       "      <td>Lord Hodge (Deputy President), Lord Briggs, Lo...</td>\n",
       "      <td>The other appeal (the \"Hooded Men case\") relat...</td>\n",
       "      <td>Lord Stephens considers each of the grounds of...</td>\n",
       "      <td>The Supreme Court unanimously allows FAAN’s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>uksc-2020-0138</td>\n",
       "      <td>Lord Reed (President), Lord Lloyd-Jones, Lady ...</td>\n",
       "      <td>In 2014, the Irish national broadcaster, RTÈ, ...</td>\n",
       "      <td>Ground 1 [86 – 91]</td>\n",
       "      <td>The Supreme Court unanimously allows the appea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>uksc-2020-0113</td>\n",
       "      <td>Lord Hodge (Deputy President), Lord Sales, Lor...</td>\n",
       "      <td>The Supreme Court allows the appeals by the Ch...</td>\n",
       "      <td>The Appellant argued that it was wrong to reca...</td>\n",
       "      <td>The Supreme Court unanimously allows the appea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>uksc-2020-0113</td>\n",
       "      <td>Lord Hodge (Deputy President), Lord Sales, Lor...</td>\n",
       "      <td>The first issue in both appeals concerns the t...</td>\n",
       "      <td>The Court rejected this interpretation of the ...</td>\n",
       "      <td>The Supreme Court unanimously allows the appea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>uksc-2020-0103</td>\n",
       "      <td>Lord Hodge (Deputy President), Lord Hamblen, L...</td>\n",
       "      <td>The Supreme Court considers and applies the te...</td>\n",
       "      <td>Ground 2 [92 – 96]</td>\n",
       "      <td>The Supreme Court unanimously dismisses the ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Case Ref                                    Names of Judges  \\\n",
       "0    uksc-2021-0160  Lord Briggs, Lady Arden, Lord Kitchin, Lord Bu...   \n",
       "1    uksc-2020-0195  Lord Reed (President), Lord Hodge (Deputy Pres...   \n",
       "2    uksc-2020-0081  Lord Reed (President), Lord Lloyd-Jones, Lady ...   \n",
       "3    uksc-2020-0029  Lord Hodge (Deputy President), Lord Lloyd-Jone...   \n",
       "4    uksc-2020-0029  Lord Hodge (Deputy President), Lord Lloyd-Jone...   \n",
       "..              ...                                                ...   \n",
       "101  uksc-2020-0206  Lord Hodge (Deputy President), Lord Briggs, Lo...   \n",
       "102  uksc-2020-0138  Lord Reed (President), Lord Lloyd-Jones, Lady ...   \n",
       "103  uksc-2020-0113  Lord Hodge (Deputy President), Lord Sales, Lor...   \n",
       "104  uksc-2020-0113  Lord Hodge (Deputy President), Lord Sales, Lor...   \n",
       "105  uksc-2020-0103  Lord Hodge (Deputy President), Lord Hamblen, L...   \n",
       "\n",
       "                                            Background  \\\n",
       "0    Mr Crosland disclosed the outcome of a judgmen...   \n",
       "1    The following issues arise. First, whether the...   \n",
       "2    Jointly, Lord Briggs, Lord Kitchin, Lord Burro...   \n",
       "3                                         Jurisdiction   \n",
       "4    The majority holds that section 13 of the Admi...   \n",
       "..                                                 ...   \n",
       "101  The other appeal (the \"Hooded Men case\") relat...   \n",
       "102  In 2014, the Irish national broadcaster, RTÈ, ...   \n",
       "103  The Supreme Court allows the appeals by the Ch...   \n",
       "104  The first issue in both appeals concerns the t...   \n",
       "105  The Supreme Court considers and applies the te...   \n",
       "\n",
       "                                               Reasons  \\\n",
       "0                                         Jurisdiction   \n",
       "1    The majority holds that section 13 of the Admi...   \n",
       "2    Lady Arden considers that the Supreme Court do...   \n",
       "3                                               Merits   \n",
       "4    The First Instance Panel made no material erro...   \n",
       "..                                                 ...   \n",
       "101  Lord Stephens considers each of the grounds of...   \n",
       "102                                 Ground 1 [86 – 91]   \n",
       "103  The Appellant argued that it was wrong to reca...   \n",
       "104  The Court rejected this interpretation of the ...   \n",
       "105                                 Ground 2 [92 – 96]   \n",
       "\n",
       "                                              Decision  \n",
       "0    Jointly, Lord Briggs, Lord Kitchin, Lord Burro...  \n",
       "1    The Supreme Court unanimously allows the appea...  \n",
       "2    \\r\\nThe Supreme Court unanimously dismisses th...  \n",
       "3    The Supreme Court allows the appeals by the Ch...  \n",
       "4    The Supreme Court allows the appeals by the Ch...  \n",
       "..                                                 ...  \n",
       "101  The Supreme Court unanimously allows FAAN’s ap...  \n",
       "102  The Supreme Court unanimously allows the appea...  \n",
       "103  The Supreme Court unanimously allows the appea...  \n",
       "104  The Supreme Court unanimously allows the appea...  \n",
       "105  The Supreme Court unanimously dismisses the ap...  \n",
       "\n",
       "[106 rows x 5 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv for subsequent use\n",
    "df5.to_csv(\"UKSC 2021-23 Additional Press Data.csv\", encoding=\"utf-8\", index=False) #removes the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is necessary to adjust the code to continue collecting data. For years 2017-2020 (4 years) much of the metadata is not available. Only the following are available:\n",
    "1. Judgment date\n",
    "2. Neutral citation number\n",
    "3. Case ID\n",
    "4. Justices\n",
    "5. Judgment PDF\n",
    "6. Press Summary (PDF) - not available in HTML unlike in 2021-2023\n",
    "\n",
    "However, the hearing dates can be extracted as the dates are provided along with the video footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.supremecourt.uk/decided-cases/2017.html\n",
      "https://www.supremecourt.uk/decided-cases/2018.html\n",
      "https://www.supremecourt.uk/decided-cases/2019.html\n",
      "https://www.supremecourt.uk/decided-cases/2020.html\n"
     ]
    }
   ],
   "source": [
    "#adjusting the code to extract links for 2017-2020\n",
    "#for loop to get links to the pages with the judgments (for each year)\n",
    "\n",
    "urlWebsite = \"https://www.supremecourt.uk/decided-cases/\" \n",
    "urls_20_17 = [] #empty list to store full url to each year\n",
    "\n",
    "#url_index = \"https://www.supremecourt.uk/decided-cases/index.html\" #includes the latest judgments\n",
    "\n",
    "for i in range(2017,2021):\n",
    "    strUrl = urlWebsite + str(i) + \".html\"\n",
    "    print(strUrl)\n",
    "    urls_20_17.append(strUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_20_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to obtain the judgments in each case in 2017-2020\n",
    "#obtaining the relative url to each case in each year\n",
    "\n",
    "case_links_20_17 = [] #empty list to store the (relative) links to each case in each year\n",
    "\n",
    "for i in urls_20_17:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    test_links = soup.find_all(class_=\"fourthColumn\")\n",
    "    #print(test_links)\n",
    "\n",
    "    for link in test_links:\n",
    "        case = link.find(\"a\", class_=\"more\") #get relative link\n",
    "        if (case != None):\n",
    "            #print(case[\"href\"])\n",
    "        case_links_20_17.append(case[\"href\"])\n",
    "        time.sleep(2)\n",
    "\n",
    "#all webpages with specific years (2022-2009) have identical html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_links_20_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the global url for each case\n",
    "\n",
    "globalurls2 = [] #empty list to store the global urls\n",
    "globurl = \"https://www.supremecourt.uk\"\n",
    "\n",
    "for i in case_links_20_17:\n",
    "    caseLink = globurl + i #appending the relative links to the global url\n",
    "    globalurls2.append(caseLink) #appending all global urls to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalurls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalurls2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting data available for the years 2017-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the years 2017-2020 have the same html code + information.\n",
    "#globalurls2 contains the case links to all cases in 2017-2020\n",
    "\n",
    "case_ref1 = [] \n",
    "first_part = \"https://www.supremecourt.uk/cases/\"\n",
    "last_part = \".html\"\n",
    "\n",
    "case_name1 = [] #empty list to store the names of the parties\n",
    "case_ID1 = [] #empty list to store the Case ID (i.e. the case in which the judgment is delivered)\n",
    "#issue = []\n",
    "#judgment_appealed = []\n",
    "justices1 = []\n",
    "hearing_start1 = []\n",
    "hearing_finish1 = []\n",
    "judgment_date1 = [] #empty list to store the date of delivery of judgment\n",
    "citation1 = []\n",
    "press_summary1 = []\n",
    "\n",
    "for i in globalurls2:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    mod_ref = i.replace(first_part, \"\").replace(last_part, \"\")\n",
    "    print(mod_ref)\n",
    "    case_ref1.append(mod_ref)\n",
    "    \n",
    "    ids = soup.find(\"h5\", text=\"Case ID\")\n",
    "    if (ids !=None):\n",
    "        for x in ids:\n",
    "            para = ids.find_next_sibling(\"p\")\n",
    "            #ids1 = x.text.strip()\n",
    "            text = para.text\n",
    "            #print(\"Case ID of\", mod_ref, \"is\", text)\n",
    "            case_ID1.append(text)\n",
    "            time.sleep(1)\n",
    "    else: \n",
    "        ids1 = soup.find(\"h3\", class_=\"sc-access\", text=\"Case ID\")\n",
    "        if (ids1 !=None):\n",
    "            for x in ids1:\n",
    "                para = ids1.find_next_sibling(\"p\")\n",
    "                text = para.text\n",
    "                #print(\"Case ID of\", mod_ref, \"is\", text)\n",
    "                case_ID1.append(text)\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                case_ID1.append(\"N/A\")\n",
    "    time.sleep(2)\n",
    "        \n",
    "    judges1 = soup.find(\"h3\", class_=\"sc-access\", text=\"Justices\")\n",
    "    if (judges1 !=None):\n",
    "        for x in judges1:\n",
    "            para = judges1.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(\"Justices names in\", mod_ref, \"are\", text)\n",
    "            justices1.append(text)\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        judges = soup.find(\"h5\", text=\"Justices\")\n",
    "        if(judges !=None):\n",
    "            for x in judges:\n",
    "                para = judges.find_next_sibling(\"p\")\n",
    "                text = para.text\n",
    "                #print(\"Justices names in\", mod_ref, \"are\", text)    \n",
    "                justices1.append(text)\n",
    "                time.sleep(1)\n",
    "        else:             \n",
    "            justices1.append(\"N/A\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    start = soup.find(\"table\", class_=\"listing video-hearing\")\n",
    "    if(start !=None):\n",
    "        tr = start.find(\"tr\", class_=\"odd\")\n",
    "        if(tr !=None):\n",
    "            td = tr.find(\"td\")\n",
    "            if (td !=None):\n",
    "                text = td.text\n",
    "                #print(\"Hearing start date in\", mod_ref, \"is\", text)    \n",
    "                hearing_start1.append(text)\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                hearing_start1.append(\"N/A\")\n",
    "        else:\n",
    "            hearing_start1.append(\"N/A\")\n",
    "    else:\n",
    "        hearing_start1.append(\"N/A\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    finish = soup.find(\"table\", class_=\"listing video-hearing\")\n",
    "    if(finish !=None):\n",
    "        tr = finish.find_all(\"tr\", class_=\"odd\")\n",
    "        if len(tr) >=2: #if there is more than 1 <tr> tag (which would be the case if the hearing went on for more than one day)\n",
    "            tr2 = tr[1] #extract the second <tr> tag\n",
    "            td = tr2.find(\"td\")\n",
    "            if (td !=None):\n",
    "                for m in td:\n",
    "                    text = m.text\n",
    "                    #print(\"Hearing finish date in\", mod_ref, \"is\", text)    \n",
    "                    hearing_finish1.append(text)\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                hearing_finish1.append(\"N/A\")\n",
    "        else:\n",
    "            hearing_finish1.append(\"N/A\")\n",
    "    else:\n",
    "        hearing_finish1.append(\"N/A\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    judgment1 = soup.find(\"h3\", class_=\"sc-access\", text=\"Judgment date\")\n",
    "    if (judgment1 != None):\n",
    "        for x in judgment1:\n",
    "            para = judgment1.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(\"Judgment date in\", mod_ref, \"is\", text)\n",
    "            judgment_date1.append(text)\n",
    "            time.sleep(1)\n",
    "    else:            \n",
    "        judgment = soup.find(\"h5\", text=\"Judgment date\")\n",
    "        if(judgment !=None):\n",
    "            for x in judgment:\n",
    "                para = judgment.find_next_sibling(\"p\")\n",
    "                text = para.text\n",
    "                #print(\"Judgment date in\", mod_ref, \"is\", text)    \n",
    "                judgment_date1.append(text)\n",
    "                time.sleep(1)\n",
    "        else:\n",
    "            judgment_date1.append(\"N/A\")\n",
    "    time.sleep(2)\n",
    "        \n",
    "    cite = soup.find(\"h5\", text=\"Neutral citation number\") #text is different\n",
    "    if(cite !=None): \n",
    "        for x in cite:\n",
    "            para = cite.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(\"Neutral citation in\", mod_ref, \"is\", text)    \n",
    "            citation1.append(text)\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        cite1 = soup.find(\"h3\", class_=\"sc-access\", text=\"Neutral citation number\")\n",
    "        if (cite1 != None):\n",
    "            para = cite1.find_next_sibling(\"p\")\n",
    "            text = para.text\n",
    "            #print(\"Neutral citation in\", mod_ref, \"is\", text)\n",
    "            citation1.append(text)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            citation1.append(\"N/A\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    press = soup.find(\"a\", title=\"Press Summary (PDF)\")\n",
    "    if (press != None):\n",
    "        #print(press[\"href\"])\n",
    "        press_link = globurl + press[\"href\"]\n",
    "        #print(press_link)\n",
    "        press_summary1.append(press_link)\n",
    "    else:\n",
    "        press_summary1.append(\"N/A\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Case References:  317\n",
      "No.of Case IDs:  374\n",
      "No.of Case Judges:  317\n",
      "No.of Hearing Start Dates:  317\n",
      "No.of Hearing End Dates:  317\n",
      "No.of Judgement Delivery Dates:  317\n",
      "No.of Case Neutral Citations:  317\n",
      "No.of Press Summary Links:  317\n"
     ]
    }
   ],
   "source": [
    "#ensure that the same no.of data for each list has been scraped.\n",
    "print(\"No.of Case References: \", len(case_ref1))\n",
    "print(\"No.of Case IDs: \", len(case_ID1))\n",
    "print(\"No.of Case Judges: \", len(justices1))\n",
    "print(\"No.of Hearing Start Dates: \", len(hearing_start1))\n",
    "print(\"No.of Hearing End Dates: \", len(hearing_finish1))\n",
    "print(\"No.of Judgement Delivery Dates: \", len(judgment_date1))\n",
    "print(\"No.of Case Neutral Citations: \", len(citation1))\n",
    "print(\"No.of Press Summary Links: \", len(press_summary1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting party names 2017-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.supremecourt.uk/decided-cases/2017.html', 'https://www.supremecourt.uk/decided-cases/2018.html', 'https://www.supremecourt.uk/decided-cases/2019.html', 'https://www.supremecourt.uk/decided-cases/2020.html']\n"
     ]
    }
   ],
   "source": [
    "print(urls_20_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract party names in each case in each year 2017-2020\n",
    "party_names1 = []\n",
    "case_ref4 = []\n",
    "first_part1 = \"/cases/\"\n",
    "last_part1 = \".html\"\n",
    "\n",
    "for i in urls_20_17:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "    \n",
    "    names = soup.find_all(class_=\"fourthColumn\")\n",
    "    for name in names:\n",
    "        name1 = name.text.strip()\n",
    "        #print(name1)\n",
    "        party_names1.append(name1)\n",
    "        time.sleep(2)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract case reference\n",
    "for i in urls_20_17:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "    \n",
    "    test_links = soup.find_all(class_=\"fourthColumn\")\n",
    "    \n",
    "    for link in test_links:\n",
    "        case = link.find(\"a\", class_=\"more\") #get relative link\n",
    "        if (case != None):\n",
    "            case2 = case[\"href\"]\n",
    "            #print(case[\"href\"])\n",
    "            mod_ref = case2.replace(first_part1, \"\").replace(last_part1, \"\")\n",
    "            #print(mod_ref)\n",
    "            case_ref4.append(mod_ref)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of Party Names:  317\n"
     ]
    }
   ],
   "source": [
    "#ensure that the same no.of data for each list has been scraped.\n",
    "#print(\"No.of Case References: \", len(case_ref4))\n",
    "print(\"No.of Party Names: \", len(party_names1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data of party names + case references of 2017-2020 in a df (df1)\n",
    "import pandas as pd\n",
    "\n",
    "#store data of 2017-2020 in a df (df1)\n",
    "import pandas as pd\n",
    "df6 = pd.DataFrame(list(zip(case_ref4, party_names1)),\n",
    "                         columns = [\"Case Ref\", \"Party Names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Ref</th>\n",
       "      <th>Party Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uksc-2016-0209</td>\n",
       "      <td>R (on the application of Hysaj and others) (Ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uksc-2016-0070</td>\n",
       "      <td>R (on the application of Black) (Appellant) v ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uksc-2016-0045</td>\n",
       "      <td>Four Seasons Holdings Incorporated (Respondent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uksc-2015-0175</td>\n",
       "      <td>Four Seasons Holdings Incorporated (Appellant)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uksc-2016-0190</td>\n",
       "      <td>CPRE Kent (Respondent) v China Gateway Interna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uksc-2016-0188</td>\n",
       "      <td>Dover District Council (Appellant) v CPRE Kent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uksc-2016-0174</td>\n",
       "      <td>O'Connor (Appellant) v Bar Standards Board (Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uksc-2016-0156</td>\n",
       "      <td>Tiuta International Limited (in liquidation) (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uksc-2017-0025</td>\n",
       "      <td>Scotch Whisky Association and others (Appellan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>uksc-2016-0142</td>\n",
       "      <td>Gordon and others, as the Trustees of the Inte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Case Ref                                        Party Names\n",
       "0  uksc-2016-0209  R (on the application of Hysaj and others) (Ap...\n",
       "1  uksc-2016-0070  R (on the application of Black) (Appellant) v ...\n",
       "2  uksc-2016-0045  Four Seasons Holdings Incorporated (Respondent...\n",
       "3  uksc-2015-0175  Four Seasons Holdings Incorporated (Appellant)...\n",
       "4  uksc-2016-0190  CPRE Kent (Respondent) v China Gateway Interna...\n",
       "5  uksc-2016-0188  Dover District Council (Appellant) v CPRE Kent...\n",
       "6  uksc-2016-0174  O'Connor (Appellant) v Bar Standards Board (Re...\n",
       "7  uksc-2016-0156  Tiuta International Limited (in liquidation) (...\n",
       "8  uksc-2017-0025  Scotch Whisky Association and others (Appellan...\n",
       "9  uksc-2016-0142  Gordon and others, as the Trustees of the Inte..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data of 2017-2020 in a df (df3)\n",
    "import pandas as pd\n",
    "df3 = pd.DataFrame(list(zip(case_ref1, case_ID1, party_names1, justices1, hearing_start1, hearing_finish1, judgment_date1, citation1)),\n",
    "                         columns = [\"Case Ref\", \"ID\", \"Party Names\", \"Names of Judges\", \"Hearing Start Date\", \"Hearing End Data\", \"Date of Judgment\", \"Citation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv for subsequent use\n",
    "df3.to_csv(\"UKSC 2017-20 Additional Complete Data.csv\", encoding=\"utf-8\", index=False) #removes the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the Press Summaries in PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.supremecourt.uk/decided-cases/2009.html\n",
      "https://www.supremecourt.uk/decided-cases/2010.html\n",
      "https://www.supremecourt.uk/decided-cases/2011.html\n",
      "https://www.supremecourt.uk/decided-cases/2012.html\n",
      "https://www.supremecourt.uk/decided-cases/2013.html\n",
      "https://www.supremecourt.uk/decided-cases/2014.html\n",
      "https://www.supremecourt.uk/decided-cases/2015.html\n",
      "https://www.supremecourt.uk/decided-cases/2016.html\n",
      "https://www.supremecourt.uk/decided-cases/2017.html\n",
      "https://www.supremecourt.uk/decided-cases/2018.html\n",
      "https://www.supremecourt.uk/decided-cases/2019.html\n",
      "https://www.supremecourt.uk/decided-cases/2020.html\n"
     ]
    }
   ],
   "source": [
    "#Judgements from 2009-2023 available online\n",
    "#for loop to get links to the pages with the judgments (for each year)\n",
    "\n",
    "urlWebsite = \"https://www.supremecourt.uk/decided-cases/\" \n",
    "urls = [] #empty list to store full url to each year\n",
    "\n",
    "#url_index = \"https://www.supremecourt.uk/decided-cases/index.html\" #includes the latest judgments\n",
    "\n",
    "for i in range(2009,2021):\n",
    "    strUrl = urlWebsite + str(i) + \".html\"\n",
    "    print(strUrl)\n",
    "    urls.append(strUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to obtain the judgments in each case:\n",
    "#obtaining the relative url to each case in each year\n",
    "\n",
    "case_links4 = [] #empty list to store the (relative) links to each case in each year\n",
    "\n",
    "for i in urls:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "\n",
    "    test_links = soup.find_all(class_=\"fourthColumn\")\n",
    "    #print(test_links)\n",
    "\n",
    "    for link in test_links:\n",
    "        case = link.find(\"a\", class_=\"more\") #get relative link\n",
    "        if (case != None):\n",
    "            #print(case[\"href\"])\n",
    "        case_links4.append(case[\"href\"])\n",
    "        time.sleep(2)\n",
    "\n",
    "#all webpages with specific years (2022-2009) have identical html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_links4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the global url for each case\n",
    "\n",
    "globalurls4 = [] #empty list to store the global urls\n",
    "globurl = \"https://www.supremecourt.uk\"\n",
    "\n",
    "for i in case_links4:\n",
    "    caseLink = globurl + i #appending the relative links to the global url\n",
    "    globalurls4.append(caseLink) #appending all global urls to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(globalurls4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#having prepared the global urls to each case, the link to the press summary of each judgment in each case is extracted: \n",
    "\n",
    "press_pdf = []\n",
    "\n",
    "for i in globalurls4:\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "       \n",
    "    text = soup.find(\"a\", title=\"Press summary (PDF)\")\n",
    "    if (text != None):\n",
    "        #print(text[\"href\"]) \n",
    "        press_pdf.append(text[\"href\"])\n",
    "    else:\n",
    "        print(\"The Press Summary (PDF) in\", text, \"is not available\")\n",
    "    time.sleep(3) #sleep is used to provide sufficient time between requests to not overload the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the global urls for the press summaries\n",
    "\n",
    "global_pressUrl = \"https://www.supremecourt.uk/cases/\"\n",
    "\n",
    "pressUrls = [] #list to store the global urls to all the judgement PDFs\n",
    "\n",
    "for i in press_pdf:\n",
    "    pressLink = global_pressUrl + i #appending the relative links to the global url\n",
    "    pressUrls.append(pressLink) #appending all global urls to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pressUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop to save the pdf files \n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "ukscP_folder = \"./UKSC_PressSum\"\n",
    "if not os.path.exists(ukscP_folder): #check if \"UKSC_PressSum folder exists\"\n",
    "    os.makedirs(ukscP_folder) #if it does not, create a new folder called titled Dataset_UKSC\n",
    "\n",
    "for i in pressUrls:\n",
    "    filename = i.split(\"/\")[-1] #[-1 indicates that the last portion divided by \"/\" is to be used to name]\n",
    "    filepath = os.path.join(ukscP_folder, filename)\n",
    "    urllib.request.urlretrieve(i, filepath)\n",
    "    time.sleep(3)\n",
    "\n",
    "#https://www.tutorialspoint.com/downloading-files-from-web-using-python\n",
    "#used to identify how to extract filename from link\n",
    "\n",
    "#https://docs.python.org/3/library/urllib.request.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "As files are in pdf format, the files need to be converted to .txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the United Kingdom Supreme Court judgments (in pdf) to .txt format\n",
    "\n",
    "pdf_folder = \"./Updated_Dataset_UKSC/\" #the pdfs are in a folder titled \"Updated_Dataset_UKSC\"\n",
    "txt_folder = \"./Updated_Dataset_UKSC_txt/\"\n",
    "if not os.path.exists(txt_folder): #check if \"Updated_Dataset_UKSC folder exists\"\n",
    "    os.makedirs(txt_folder) #if it does not, create a new folder called titled Updated_Dataset_UKSC\n",
    "\n",
    "#iterate over all the PDF files in the directory\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith('.pdf'):\n",
    "        #open PDF file in read mode\n",
    "        pdf_file = open(os.path.join(pdf_folder, filename), 'rb')\n",
    "\n",
    "        #create a PDF reader object\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "\n",
    "        #get the number of pages in the PDF file\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        #create a text file for each PDF file\n",
    "        txt_file_path = os.path.join(txt_folder, '{}.txt'.format(filename[:-4])) #filename is the same (.pdf deleted and .txt added)\n",
    "        txt_file = open(txt_file_path, 'w', encoding=\"utf-8\")\n",
    "        \n",
    "        #write the text content of each page to the text file\n",
    "        for page in range(num_pages):\n",
    "            page_text = pdf_reader.pages[page].extract_text()\n",
    "       \n",
    "            #write the text content to the text file\n",
    "            txt_file.write(page_text)\n",
    "\n",
    "        #close the text file and PDF file\n",
    "        txt_file.close()\n",
    "        pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use PDFMiner to convert texts to .txt files\n",
    "def text_process(pdf_folder, txt_folder):\n",
    "    text = extract_text(pdf_folder)\n",
    "    \n",
    "    with open(txt_folder, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "        txt_file.write(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_folder = \"Updated_Dataset_UKSC\"\n",
    "    txt_folder = \"new UKSC PDFMiner\"\n",
    "    \n",
    "    if not os.path.exists(txt_folder):\n",
    "        os.makedirs(txt_folder)\n",
    "        \n",
    "     #iterate through all PDFs in the input folder\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            input_pdf_path = os.path.join(pdf_folder, filename)\n",
    "            output_txt_path = os.path.join(txt_folder, os.path.splitext(filename)[0] + '.txt')\n",
    "            text_process(pdf_folder, output_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UKSC Press Summary Dataset: Text Extraction using PDFMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_folder = \"./UKSC_PressSum/\"\n",
    "    txt_folder = \"./Dataset_UKSC-PressSum_PDFMiner/\"\n",
    "    \n",
    "    if not os.path.exists(txt_folder):\n",
    "        os.makedirs(txt_folder)\n",
    "        \n",
    "    text_process(pdf_folder, txt_folder)\n",
    "    \n",
    "#uksc-2013-0030 was removed - PDF corrupted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
